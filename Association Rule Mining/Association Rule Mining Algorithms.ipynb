{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory of Apriori Algorithm\n",
    "\n",
    "Association rule mining (ARM) is a useful data mining technique to detect patterns and rules. Apriori is an applied ARM algorithm to find rules and relations among variables in a data set. There are three major components of Apriori algorithm:\n",
    "* Support\n",
    "* Confidence\n",
    "* Lift\n",
    "\n",
    "### Support\n",
    "Support refers to the default popularity of an item and can be calculated by finding number of transactions containing a particular item divided by total number of transactions. Suppose we want to find support for item B. This can be calculated as:\n",
    "\n",
    "Support(B) = (Transactions containing (B))/(Total Transactions)  \n",
    "\n",
    "### Confidence\n",
    "Confidence refers to the likelihood that an item B is also bought if item A is bought. It can be calculated by finding the number of transactions where A and B are bought together, divided by total number of transactions where A is bought. Mathematically, it can be represented as:\n",
    "\n",
    "Confidence(A→B) = (Transactions containing both (A and B))/(Transactions containing A) \n",
    "\n",
    "### Lift\n",
    "Lift(A -> B) refers to the increase in the ratio of sale of B when A is sold. Lift(A –> B) can be calculated by dividing Confidence(A -> B) divided by Support(B). Mathematically it can be represented as:\n",
    "\n",
    "Lift(A→B) = (Confidence (A→B))/(Support (B))  \n",
    "\n",
    "### Steps Involved in Apriori Algorithm\n",
    "For large sets of data, there can be hundreds of items in hundreds of thousands transactions. The Apriori algorithm tries to extract rules for each possible combination of items. This process can be extremely slow due to the number of combinations. To speed up the process, we need to perform the following steps:\n",
    "1.\tSet a minimum value for support and confidence. This means that we are only interested in finding rules for the items that have certain default existence (e.g. support) and have a minimum value for co-occurrence with other items (e.g. confidence).\n",
    "2.\tExtract all the subsets having higher value of support than minimum threshold.\n",
    "3.\tSelect all the rules from the subsets with confidence value higher than minimum threshold.\n",
    "4.\tOrder the rules by descending order of Lift.\n",
    "\n",
    "### Example 1 -- Generating Association Rules from Frequent Itemsets\n",
    "The generate_rules takes dataframes of frequent itemsets as produced by the apriori function in mlxtend.association. To demonstrate the usage of the generate_rules method, we first create a pandas DataFrame of frequent itemsets as generated by the apriori function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "\n",
    "\n",
    "dataset = [['Milk', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt'],\n",
    "           ['Dill', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt'],\n",
    "           ['Milk', 'Apple', 'Kidney Beans', 'Eggs'],\n",
    "           ['Milk', 'Unicorn', 'Corn', 'Kidney Beans', 'Yogurt'],\n",
    "           ['Corn', 'Onion', 'Onion', 'Kidney Beans', 'Ice cream', 'Eggs']]\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(dataset).transform(dataset)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "frequent_itemsets = apriori(df, min_support=0.6, use_colnames=True)\n",
    "\n",
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generate_rules() function allows you to (1) specify your metric of interest and (2) the according threshold. Currently implemented measures are confidence and lift. Let's say you are interesting in rules derived from the frequent itemsets only if the level of confidence is above the 90 percent threshold (min_threshold=0.7):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2 -- Selection Criteria\n",
    "If you are interested in rules according to a different metric of interest, you can simply adjust the metric and min_threshold arguments . E.g. if you are only interested in rules that have a lift score of >= 1.2, you would do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.2)\n",
    "rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas DataFrames make it easy to filter the results further. Let's say we are ony interested in rules that satisfy the following criteria:\n",
    "\n",
    "1. at least 2 antecedents\n",
    "2. a confidence > 0.75\n",
    "3. a lift score > 1.2\n",
    "\n",
    "We could compute the antecedent length as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rules[\"antecedent_len\"] = rules[\"antecedents\"].apply(lambda x: len(x))\n",
    "rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can use pandas' selection syntax as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rules[ (rules['antecedent_len'] >= 2) &\n",
    "       (rules['confidence'] > 0.75) &\n",
    "       (rules['lift'] > 1.2) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, using the Pandas API, we can select entries based on the \"antecedents\" or \"consequents\" columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rules[rules['antecedents'] == {'Eggs', 'Kidney Beans'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frozensets\n",
    "\n",
    "Note that the entries in the \"itemsets\" column are of type frozenset, which is built-in Python type that is similar to a Python set but immutable, which makes it more efficient for certain query or comparison operations (https://docs.python.org/3.6/library/stdtypes.html#frozenset). Since frozensets are sets, the item order does not matter. I.e., the query\n",
    "\n",
    "rules[rules['antecedents'] == {'Eggs', 'Kidney Beans'}]\n",
    "\n",
    "is equivalent to any of the following three\n",
    "\n",
    "* rules[rules['antecedents'] == {'Kidney Beans', 'Eggs'}]\n",
    "* rules[rules['antecedents'] == frozenset(('Eggs', 'Kidney Beans'))]\n",
    "* rules[rules['antecedents'] == frozenset(('Kidney Beans', 'Eggs'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3 -- Frequent Itemsets with Incomplete Antecedent and Consequent Information\n",
    "Most metrics computed by association_rules depends on the consequent and antecedent support score of a given rule provided in the frequent itemset input DataFrame. Consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dict = {'itemsets': [['177', '176'], ['177', '179'],\n",
    "                     ['176', '178'], ['176', '179'],\n",
    "                     ['93', '100'], ['177', '178'],\n",
    "                     ['177', '176', '178']],\n",
    "        'support':[0.253623, 0.253623, 0.217391,\n",
    "                   0.217391, 0.181159, 0.108696, 0.108696]}\n",
    "\n",
    "freq_itemsets = pd.DataFrame(dict)\n",
    "freq_itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is a \"cropped\" DataFrame that doesn't contain the support values of the item subsets. This can create problems if we want to compute the association rule metrics for, e.g., 176 => 177.\n",
    "For example, the confidence is computed as confidence(A→C)=support(A→C)/support(A). But we do not have support(A). All we know about \"A\"'s support is that it is at least 0.253623.\n",
    "\n",
    "In these scenarios, where not all metric's can be computed, due to incomplete input DataFrames, you can use the support_only=True option, which will only compute the support column of a given rule that does not require as much info:\n",
    "\n",
    "support(A→C)=support(A∪C),\n",
    "\n",
    "\"NaN's\" will be assigned to all other metric columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "res = association_rules(freq_itemsets, support_only=True, min_threshold=0.1)\n",
    "res "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To clean up the representation, you may want to do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = res[['antecedents', 'consequents', 'support']]\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API\n",
    "association_rules(df, metric='confidence', min_threshold=0.8, support_only=False)\n",
    "\n",
    "Generates a DataFrame of association rules including the metrics 'score', 'confidence', and 'lift'\n",
    "\n",
    "#### Parameters\n",
    "\n",
    "df : pandas DataFrame\n",
    "\n",
    "pandas DataFrame of frequent itemsets with columns ['support', 'itemsets']\n",
    "\n",
    "metric : string (default: 'confidence')\n",
    "\n",
    "Metric to evaluate if a rule is of interest. Automatically set to 'support' if support_only=True. Otherwise, supported metrics are 'support', 'confidence', 'lift',\n",
    "\n",
    "'leverage', and 'conviction' These metrics are computed as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- support(A->C) = support(A+C) [aka 'support'], range: [0, 1]\n",
    "\n",
    "- confidence(A->C) = support(A+C) / support(A), range: [0, 1]\n",
    "\n",
    "- lift(A->C) = confidence(A->C) / support(C), range: [0, inf]\n",
    "\n",
    "- leverage(A->C) = support(A->C) - support(A)*support(C),\n",
    "range: [-1, 1]\n",
    "\n",
    "- conviction = [1 - support(C)] / [1 - confidence(A->C)],\n",
    "range: [0, inf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* min_threshold : float (default: 0.8)\n",
    "\n",
    "Minimal threshold for the evaluation metric, via the metric parameter, to decide whether a candidate rule is of interest.\n",
    "\n",
    "* support_only : bool (default: False)\n",
    "\n",
    "Only computes the rule support and fills the other metric columns with NaNs. This is useful if:\n",
    "\n",
    "a) the input DataFrame is incomplete, e.g., does not contain support values for all rule antecedents and consequents\n",
    "\n",
    "b) you simply want to speed up the computation because you don't need the other metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Returns\n",
    "\n",
    "pandas DataFrame with columns \"antecedents\" and \"consequents\" that store itemsets, plus the scoring metric columns: \"antecedent support\", \"consequent support\", \"support\", \"confidence\", \"lift\", \"leverage\", \"conviction\" of all rules for which metric(rule) >= min_threshold. Each entry in the \"antecedents\" and \"consequents\" columns are of type frozenset, which is a Python built-in type that behaves similarly to sets except that it is immutable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
